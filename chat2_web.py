import os
import torch
import torchaudio
import gradio as gr
import time
import base64
import whisper
import numpy as np
import requests
import json
from datetime import datetime
from pathlib import Path
from generator import Segment, Generator, load_csm_1b
from io import BytesIO

# åˆ›å»º wav ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs('wav', exist_ok=True)

# åˆå§‹åŒ–è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# åˆå§‹åŒ– Whisper æ¨¡å‹ç”¨äºè¯­éŸ³è¯†åˆ«
print("Loading Whisper model...")
whisper_model = whisper.load_model("turbo")
print("Whisper model loaded")

# åˆå§‹åŒ– CSM æ¨¡å‹ç”¨äºè¯­éŸ³ç”Ÿæˆ
print("Loading CSM model...")
model_path = "/root/autodl-tmp/csm/model/ckpt.pt"
generator = load_csm_1b(model_path, device)
print("CSM model loaded")

# DeepSeek é…ç½®
DEEPSEEK_API_KEY = "sk-"
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# åŠ è½½éŸ³é¢‘å‡½æ•°
def load_audio(audio_path, target_sample_rate):
    audio_tensor, sample_rate = torchaudio.load(audio_path)
    audio_tensor = torchaudio.functional.resample(
        audio_tensor.squeeze(0), orig_freq=sample_rate, new_freq=target_sample_rate
    )
    return audio_tensor

# è°ƒç”¨ DeepSeek Chat API ç”Ÿæˆæ–‡æœ¬å›å¤
def get_deepseek_response(user_message, history):
    try:
        print(f"Calling DeepSeek API for user message: '{user_message}'")
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        for item in history:
            messages.append({"role": item["role"], "content": item["content"]})
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        # ç³»ç»Ÿæç¤ºè¯ï¼šç®€çŸ­æ¸©æŸ”çš„å¥³ç”Ÿè®¾å®š
        system_prompt = """You are a gentle and lovely girl, we are in a relationship of lovers, in communication:
1. The reply should be short, preferably no more than 30 words
2. The tone should be soft and sweet, full of emotion.
3. Reply in English, no emoticons allowed
You're fun to be around, a little provocative
6. Do not display your reply in the form of AI.
7. Ensure that users feel present and immersed."""
                
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        payload = {
            "model": "deepseek-chat",
            "messages": messages,
            "system": system_prompt,
            "temperature": 0.9,  # å¢åŠ åˆ›æ„æ€§
            "max_tokens": 100,    # é™åˆ¶è¾“å‡ºé•¿åº¦
            "top_p": 0.9        # ä¿æŒé€‚åº¦çš„éšæœºæ€§
        }
        
        # å‘é€è¯·æ±‚åˆ° DeepSeek API
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
        }
        
        response = requests.post(
            f"{DEEPSEEK_BASE_URL}/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result["choices"][0]["message"]["content"]
            print(f"DeepSeek response: '{ai_response}'")
            return ai_response
        else:
            print(f"DeepSeek API error: {response.status_code}")
            print(response.text)
            return f"æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›åº”ï¼Œè¯·ç¨åå†è¯•ã€‚"
            
    except Exception as e:
        print(f"Error calling DeepSeek API: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜~"

# åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨
class ConversationManager:
    def __init__(self, max_history=5):
        self.max_history = max_history
        self.history = []
        self.chatbot_history = []
        
        # å‡†å¤‡å‚è€ƒè¯­éŸ³æ®µè½ï¼ˆä½¿ç”¨å¥³å£°ä½œä¸ºå‚è€ƒï¼‰
        self.reference_text = "And Lake turned round upon me, a little abruptly, with his odd yellowish eyes, a little like those of the sea eagle, and the ghost of his smile that flickered on his singularly pale face with a stern and insidious look confronted me."
        self.reference_path = "/root/autodl-tmp/csm/model/prompts/read_speech_a.wav"  # å¥³å£°å‚è€ƒ
        
        # åŠ è½½å‚è€ƒè¯­éŸ³
        self.reference_segment = Segment(
            text=self.reference_text,
            speaker=0,  # å¥³å£°
            audio=load_audio(self.reference_path, generator.sample_rate)
        )
        
        print(f"å·²åˆå§‹åŒ–å¥³å£°å‚è€ƒè¯­éŸ³: {self.reference_text}")
    
    def add_message(self, text, audio_tensor, is_user=True):
        speaker_id = 1 if is_user else 0
        segment = Segment(text=text, speaker=speaker_id, audio=audio_tensor)
        
        self.history.append(segment)
        
        # æ›´æ–°èŠå¤©æœºå™¨äººå†å²ä¸ºæ–°çš„æ¶ˆæ¯æ ¼å¼
        message = {
            "role": "user" if is_user else "assistant",
            "content": text
        }
        self.chatbot_history.append(message)
        
        # ä¿æŒæœ€å¤§å†å²è®°å½•é•¿åº¦
        if len(self.history) > self.max_history * 2:  # æ¯è½®å¯¹è¯åŒ…å«ç”¨æˆ·å’Œç³»ç»Ÿæ¶ˆæ¯
            self.history = self.history[-self.max_history * 2:]
            self.chatbot_history = self.chatbot_history[-self.max_history * 2:]
            
        return segment
    
    def get_context(self):
        # è¿”å›å‚è€ƒè¯­éŸ³æ®µè½åŠ ä¸Šå†å²è®°å½•
        return [self.reference_segment] + self.history
    
    def get_chatbot_history(self):
        # ç›´æ¥è¿”å›æ¶ˆæ¯å†å²è®°å½•åˆ—è¡¨
        return self.chatbot_history

# åˆå§‹åŒ–å¯¹è¯ç®¡ç†å™¨
conv_manager = ConversationManager()

# ä½¿ç”¨Whisperå°†ç”¨æˆ·éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬
def transcribe_audio(audio_path):
    print(f"è½¬å†™éŸ³é¢‘: {audio_path}")
    try:
        result = whisper_model.transcribe(audio_path)
        transcribed_text = result["text"]
        print(f"è½¬å†™ç»“æœ: '{transcribed_text}'")
        return transcribed_text
    except Exception as e:
        print(f"è½¬å†™è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return f"[è½¬å†™é”™è¯¯]"




# ç”Ÿæˆç³»ç»Ÿå›å¤-------------------------------------------------------------------------------------------------
def generate_response(user_text, user_audio_tensor):
    print(f"å¤„ç†ç”¨æˆ·è¾“å…¥: '{user_text}'")
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
    user_segment = conv_manager.add_message(user_text, user_audio_tensor, is_user=True)
    
    # ä½¿ç”¨DeepSeekç”ŸæˆAIå›å¤æ–‡æœ¬
    system_text = get_deepseek_response(user_text, conv_manager.chatbot_history[:-1])
    
    print(f"ä¸ºAIå›å¤ç”Ÿæˆè¯­éŸ³: '{system_text}'")
    
    # è·å–å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå‚è€ƒè¯­éŸ³ + å†å²è®°å½•ï¼‰
    context = conv_manager.get_context()
    
    # ç”Ÿæˆç³»ç»Ÿå›å¤çš„è¯­éŸ³
    system_audio = generator.generate(
        text=system_text,  # ç³»ç»Ÿå›å¤æ–‡æœ¬
        speaker=0,         # ä½¿ç”¨å¥³å£°
        context=context,   # åŒ…å«å‚è€ƒè¯­éŸ³å’Œå¯¹è¯å†å²çš„ä¸Šä¸‹æ–‡
        max_audio_length_ms=20_000,  # å¢åŠ æœ€å¤§é•¿åº¦ä»¥é€‚åº”è¾ƒé•¿å›å¤
    )
    
    # æ·»åŠ ç³»ç»Ÿå›å¤åˆ°å†å²è®°å½•
    system_segment = conv_manager.add_message(system_text, system_audio, is_user=False)
    
    # å°†éŸ³é¢‘è½¬æ¢ä¸º16ä½æ•´æ•°æ ¼å¼
    audio_numpy = (system_audio.cpu().numpy() * 32767).astype(np.int16)
    
    # ä½¿ç”¨BytesIOå’Œsoundfileä¿å­˜éŸ³é¢‘
    wav_io = BytesIO()
    import soundfile as sf
    sf.write(wav_io, audio_numpy, samplerate=generator.sample_rate, format="WAV")
    wav_io.seek(0)
    wav_bytes = wav_io.getvalue()
    
    # å°†WAVæ•°æ®è½¬æ¢ä¸ºbase64ç¼–ç 
    audio_b64 = base64.b64encode(wav_bytes).decode('utf-8')
    
    # åˆ›å»ºä¸€ä¸ªå¸¦æœ‰è‡ªåŠ¨æ’­æ”¾çš„HTMLéŸ³é¢‘å…ƒç´ 
    html_audio = f'<audio controls autoplay src="data:audio/wav;base64,{audio_b64}"></audio>'
    
    print(f"ç”Ÿæˆçš„ç³»ç»Ÿå›å¤éŸ³é¢‘é•¿åº¦: {len(audio_numpy)} é‡‡æ ·ç‚¹")
    
    # è¿”å›ç³»ç»Ÿå›å¤æ–‡æœ¬ã€HTMLéŸ³é¢‘å…ƒç´ å’Œæ ¼å¼åŒ–çš„å¯¹è¯å†å²
    return system_text, gr.HTML(html_audio), conv_manager.get_chatbot_history()

# æ¸…é™¤å¯¹è¯å†å²
def clear_history():
    global conv_manager
    conv_manager = ConversationManager()
    return "", gr.HTML(""), []  # è¿”å›ç©ºçš„èŠå¤©å†å²

# å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆéŸ³é¢‘å½•åˆ¶ï¼‰
def process_user_input(audio_file):
    if audio_file is None:
        return "è¯·å…ˆå½•åˆ¶éŸ³é¢‘æ¶ˆæ¯ã€‚", gr.HTML(""), []
    
    # åŠ è½½å¹¶è½¬æ¢ç”¨æˆ·éŸ³é¢‘
    audio_tensor, sample_rate = torchaudio.load(audio_file)
    audio_tensor = torchaudio.functional.resample(
        audio_tensor.squeeze(0), orig_freq=sample_rate, new_freq=generator.sample_rate
    )
    
    # ä½¿ç”¨Whisperå°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºWhisperå¤„ç†
    temp_path = "temp_whisper_input.wav"
    torchaudio.save(temp_path, audio_tensor.unsqueeze(0), generator.sample_rate)
    user_text = transcribe_audio(temp_path)
    os.remove(temp_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    
    # ç”Ÿæˆç³»ç»Ÿå“åº”
    return generate_response(user_text, audio_tensor)

# åˆ›å»ºGradioç•Œé¢
demo = gr.Blocks(theme=gr.themes.Soft())
with demo:
    gr.Markdown("# ğŸ™ï¸ AIè¯­éŸ³å¯¹è¯ç³»ç»Ÿ")
    
    with gr.Row():
        with gr.Column(scale=1):
            audio_input = gr.Audio(
                sources=["microphone"], 
                type="filepath",
                label="å½•åˆ¶æ¶ˆæ¯"
            )
            submit_btn = gr.Button("å‘é€", variant="primary")
            clear_btn = gr.Button("æ¸…é™¤å¯¹è¯å†å²", variant="secondary")
            
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(
                label="å¯¹è¯å†å²", 
                height=400,
                show_label=True,
                type="messages"
            )
            with gr.Row():
                system_text = gr.Textbox(
                    label="AIå›å¤æ–‡æœ¬", 
                    interactive=False
                )
            audio_output = gr.HTML(
                label="AIè¯­éŸ³å›å¤"
            )
    
    # è®¾ç½®äº‹ä»¶å¤„ç†
    submit_btn.click(
        fn=process_user_input,
        inputs=[audio_input],
        outputs=[system_text, audio_output, chatbot]
    )
    
    # æ¸…é™¤å¯¹è¯å†å²
    def clear_history():
        global conv_manager
        conv_manager = ConversationManager()
        return "", gr.HTML(""), []
    
    clear_btn.click(
        fn=clear_history,
        inputs=[],
        outputs=[system_text, audio_output, chatbot]
    )

    # è‡ªå®šä¹‰CSSæé«˜ç•Œé¢ç¾è§‚åº¦
    demo.load(js="""
    function checkAudioVisibility() {
        const audioResponse = document.getElementById('audio-response');
        if (audioResponse) {
            const waveform = audioResponse.querySelector('.audio-waveform');
            if (waveform) {
                waveform.style.display = 'block';
                waveform.style.height = '60px';
            }
        }
        setTimeout(checkAudioVisibility, 1000);
    }
    document.addEventListener('DOMContentLoaded', checkAudioVisibility);
    """)

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs("gradio_cache", exist_ok=True)
    demo.launch(server_name="0.0.0.0", server_port=6006, share=True)